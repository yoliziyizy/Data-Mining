---
title: "Credit Risk via Lending Club"
author: " Modern Data Mining"
date: " "
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 4)
if(!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, ggplot2, glmnet, car, data.table)   #add your packages here
```



```{r echo=FALSE}
loan <- fread("/Users/ziyi/Desktop/Penn/spring_2022/STAT571/571 Homework 4/LendingClubCase/LoanStats_07_11_Clean.csv", stringsAsFactors = T)
```
# Instructions

* This is a project. Well orgnized and well presented write-up is one major motivation here. Please see the section on `Write up` for details. 
* There is no single correct answer.  
* The entire write up should not be more than **5** pages. All the R-codes should be hidden. Any R-output used should be formatted neatly. You may put all supporting documents, graphics, or other exhibits into an Appendix, which is not counted in the 5 page limit.

# Introduction


## Background
Lending Club https://www.lendingclub.com/ is the world’s largest peer-to-peer online platform connecting borrowers and investors. By cutting off the middle financial institutions in traditional lending process, Lending Club provides higher return for individual investors and lower interest rates for borrowers to get access to funding.

Treasury bonds are always considered to be risk-free since they are backed by the U.S. government. However, the interest rate of 10-year T-bond decreased from 4.68% in 2007 to 1.97% in 2015, and many investors are trying to find better alternative choices for investment. Lending Club offers an attractive choice. As such, Lending Club’s business has grown exponentially in the loan market in recent years, so it’s more and more important for Lending Club to control risks by distinguishing good (not default, borrowers pay back loans monthly) and bad (default, borrowers do not pay back) loans. 

In the dataset provided, only the applications that were approved by Lending Club and provided a loan are included. This means that Lending Club has already filtered out some of the applicants based on certain criteria. We will restrict our analysis to the period between 2007-2011 for which we have around 39,000 observations and 38 attributes for each of these loans. These attributes include loan amount, home ownership status, interest rate on the loan, loan status and grade of the loan among many others.

Data needed: 

- **LoanStats_07_11_Full.csv**
- **LoanStats_07_11_Clean.csv**


## Goals of the study

1. You are hired by a large investment firm to invest their money through Lending Club. You are going to apply the machine learning skills from our data mining class and recommend a classification rule to identify types of loans to be included in the portfolio. In particular, you need to provide the following information

i)  First, identify the important risk factors that a loan will be defaulted. 

- This will require you to report a model to characterize the relationship between risk factors to the chance of a loan being defaulted. 

- The set of available predictors is not limited to the raw variables in the data set. You may engineer any factors using the data that you think will improve your model's quality.


ii)  Build a classifier that maximizes the return. 

- To do so you need to propose a sensible loss ratio for picking up a bad loan. We did a quick estimate that the loss ratio of picking up a bad loan to that of missing a good loan is about 2 to 1. You may modify this loss ratio with your reasoning. 

- You may build a few models possibly using elastic net, direct logistic regression etc. and choose among them. Your final classifier built should have a good testing power. I.e., it should work well for a testing data set that you may reserve from the data provided. 

- Notice that the model behind the classifier you build here may differ from that constructed in question i).

2. You are young and ambitious. You see the opportunity of getting into this business. Based on the information available from the Lending club website and the analyses you have done, summarize why the lending club is so successful and has been able to grow the business rapidly. To do so you may need to gather further information to better understand Lending Club's business model.

3. Based on your knowledge gathered so far and analyses you have done, what can you offer to the Lending Club so that they can modify their selection rules to increase the returns for investors. [You may need go beyond the dataset provided.]

4. You may propose your own goal of study and ignore the agenda proposed above!

5. We suggest you to split the data first to Training/Testing/Validation data:

- Use training/testing data to land a final model (If you only use LASSO to land a final model, we will not need testing data since all the decisions are made with cross-validations.)

- Evaluate the final model with the validation data to give an honest assessment of your final model. 




### Characteristics of the Data Set

The data ranges from 2007 to 2011. We do not include data after 2012 because there might be loans that have not been closed. The original dataset is from https://www.lendingclub.com/info/download-data.action (sorry no longer available) and we drop irrelevant variables and variables with lots of missing values.


### Description of variables

The variables could broadly be segmented into pre-funded loan data, borrower data, borrower credit data and post-loan data. 

**a) Pre-funded loan data **
a.`loan_amnt`: The listed amount of the loan applied for by the borrower
b. `int_rate`: Interest Rate on the loan
c. `grade`: LC assigned loan grade
d. `sub_grade` : LC assigned loan subgrade
e. `installment`: The monthly payment owed by the borrower if the loan originates
f. `purpose`: The monthly payment owed by the borrower if the loan originates.
g. `term`: The number of payments on the loan. Values are in months and can be either 36 or 60.
 
Remark: LC Grade

Loans **grade** tranches "credit-worthy" borrowers into seven investment grades, each with five subgrades (for a total of 35 tranches). Lending Club's methodology for tranching its borrowers follows proprietary scoring models based on FICO scores, credit history, "certain other credit attributes", loan term and loan amount.

Investors can decide how much to fund each borrower (subject to a minimum investment amount of $25/loan) and the proportion to invest in different loans of grade.

For instance, a loan in tranche A, the highest-rated tranche may default while a loan in tranche E, the lowest-rated tranche, may pay in full. To be sure, variance is expected. No model is perfect. However, the variance made us realize that there were some assets that were priced inefficiently and thus, there existed an abritrage opportunity. If we could more accurately identify the loans that are likely to default and those that are likely to pay in full, then we could profit disproportionately.

Loan | Grade	Interest | Rate	Origination | Fee	36-Month | APR	60-Month | APR
---- | ---- | ---- | ---- | ---- 
  A | 5.32% - 7.97%	| 1% - 5% |	5.99% - 11.49%	| 7.46% - 10.17%
  B	| 9.44% - 11.99%	| 5%	| 12.99% - 15.59%	| 11.67% - 14.27%
C	| 12.62% - 16.02%	| 6%	| 16.99% - 20.49%	| 15.40% - 18.89%
D	| 17.09% - 21.45%	| 6%	| 21.59% - 26.07%	| 19.99% - 24.48%
E	| 19.99% - 26.30%	| 6%	| 24.57% - 31.06%	| 22.98% - 29.49%
F	| 24.24% - 30.75%	| 6%	| 28.94% - 35.64%	| 27.36% - 34.09%
G	| 28.55% - 30.99%	| 6%	| 33.37% - 35.89%	| 31.82% - 34.34%

**b) Borrower basic information**
a. `emp_title`: The job title supplied by the Borrower when applying for the loan
b. `emp_length`: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. 
c. `home_ownership`: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER.
d. `annual_inc`: The self-reported annual income provided by the borrower during registration
e. `zip_code`: The first 3 numbers of the zip code provided by the borrower in the loan application
f. `addr_state`: The state provided by the borrower in the loan application
g. `verification_status`: Indicates if income was verified by LC, not verified, or if the income source was verified


**c) Borrower credit data**
a. `dti`: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.
b. `delinq_2yrs`: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years
c. `earliest_cr_line`: The month the borrower's earliest reported credit line was opened
d. `inq_last_6mths`: The number of inquiries in past 6 months (excluding auto and mortgage inquiries)
e. `open_acc`: The number of open credit lines in the borrower's credit file.
f. `pub_rec`: Number of derogatory public records
g. `revol_bal`: Total credit revolving balance
h. `revol_util`: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.
i. `total_acc`: The total number of credit lines currently in the borrower's credit file
j. `pub_rec_bankruptcies`: Number of public record bankruptcies

Credit data of the borrower, such as their debt to income ratio, delinquencies, credit inquiries, etc. 

**d) Post-loan data**
a. `issue_d`: The month which the loan was funded 
b. `loan_status`: Current status of the loan
c. `funded_amnt`: The total amount committed to that loan at that point in time.
d. `funded_amnt_inv`: The total amount committed by investors for that loan at that point in time
e. `total_pymnt`: Payments received to date for total amount funded
f. `total_pymnt_inv`: Payments received to date for portion of total amount funded by investors
g. `total_rec_prncp`: Principal received to date
h. `total_rec_int`: Interest received to date
i. `total_rec_late_fee`: Late fees received to date
j. `recoveries`: post charge off gross recovery
k. `collection_recovery_fee`: post charge off collection fee
l. `last_pymnt_d`: Last month payment was received
m. `last_pymnt_amnt`: Last total payment amount received
n. `last_credit_pull_d`: The most recent month LC pulled credit for this loan

Including post-loan data is unrealistic to predict the default rate *before* making the investment!

### Response: `loan_status`
loan_status | Description
----------- | ------------------------------------------------------------------------
Defaulted | Overdue for over 90 days
Charged off | defaulted and there is no longer a reasonable expectation of further payments (e.g. bankruptcy). 

Fully Paid  | 

To save your time we are going to use some data sets cleaned by us. Thus, we provide two datasets:

**`LoanStats_07_11_Full.csv`** is the original data. You may use it for the purpose of summary if you wish. You will see that the original data can’t be used directly for your analysis, yet. 

**`LoanStats_07_11_Clean.csv`** is a cleaned version and they are modified in the following ways:

1) Columns with lots of NAs are excluded.

2) `pymnt_plan`, `out_prncp`, `out_prncp_inv`, `collections_12_mths_ex_med`, `chargeoff_within_12_mths`, `tax_liens`, `initial_list_status`, `application_type`, `policy_code` have little variability, and are as such excluded.

3) Drop `title` since it has similar explanatory value as `purpose`.

4) Drop `emp_title` because it has too many levels, but it is possible to classify them into different sectors.

5) Drop all rows with NAs directly after 1)-4) at the expense of 2% loss of data.

6) You might include `desc` back in the data to do text mining.

# Suggested outline for your report

As you all know, it is very important to present your findings well. To achieve the best possible results you need to understand your audience. 

Your target audience is a manager who holds an MBA, is familiar with financial terminology, and has gone through a similar course to our Modern Data Mining with someone like your professor. You can thus assume some level of technical familiarity, but should not let the paper be bogged down with code or other difficult to understand output.

Note then that the most important elements of your report are the clarity of your analysis and the quality of your proposals. 

A suggested outline of the report would include the following components: 

1) Executive Summary

* This section should be accessible by people with very little statistical background (avoid using technical words and no direct R output is allowed)
* Give a background of the study. You may check the original website or other sources to fill in some details, such as to why the questions we address here are important. 
* A quick summary about the data.
* Methods used and the main findings.
* You may use clearly labelled and explained visualizations.
* Issues, concerns, limitations of the conclusions. This is an especially important section to be honest in - we might be Penn students, but we are statisticians today.


First read in the data.

```{r}
datafull <- read.csv("/Users/ziyi/Desktop/Penn/spring_2022/STAT571/571 Homework 4/LendingClubCase/LoanStats_07_11_Full.csv")
data <- read.csv("/Users/ziyi/Desktop/Penn/spring_2022/STAT571/571 Homework 4/LendingClubCase/LoanStats_07_11_Clean.csv",  stringsAsFactors = T)
names(data)
dim(data)
```

Drop `emp_title` because it has too many levels.

```{r}
data1 <- data %>% select(-emp_title) # 38971 observations, 37 variables, no missing values
# skimr::skim(data1)
# summary(data1)
```

We have 33503 observations with fully paid loan status and 5468 observations with charged off loan status.

2) Detailed process of the analysis

i) Data Summary /EDA

* Nature of the data, origin
* Necessary quantitative and graphical summaries
* Are there any problems with the data?
* Which variables are considered as input 

```{r}
data1 %>% group_by(loan_status)  %>% summarise(mean(loan_amnt))
```

```{r}
data1 %>%
  ggplot(aes(x = grade, fill = grade)) + 
  geom_bar(stat = "count") + 
  labs(title = "LC Grade Distribution") + xlab("LC Grade") + ylab("Number of samples")
```

```{r}
data1 %>%
  group_by(grade, loan_status) %>%
  summarise(num = n()) %>%
  ggplot(aes(x = grade, y = num, fill = loan_status)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Loan Status by LC Grade") + xlab("LC Grade") + ylab("Number of samples")
```

```{r}
data1 %>%
  mutate(sub_grade = sub("A", "", sub_grade)) %>%
  mutate(sub_grade = sub("B", "", sub_grade)) %>%
  mutate(sub_grade = sub("C", "", sub_grade)) %>%
  mutate(sub_grade = sub("D", "", sub_grade)) %>%
  mutate(sub_grade = sub("E", "", sub_grade)) %>%
  mutate(sub_grade = sub("F", "", sub_grade)) %>%
  mutate(sub_grade = sub("G", "", sub_grade)) %>%
  group_by(grade, sub_grade, loan_status) %>%
  summarise(num = n()) %>%
  ggplot(aes(x = sub_grade, y = num, fill = loan_status)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(.~grade) +
  labs(title = "Loan Status by LC Sub-grade") + xlab("LC Sub-grade") + ylab("Number of samples")
```

```{r}
data1 %>%
  group_by(home_ownership, loan_status) %>%
  ggplot(aes(x = home_ownership, fill = loan_status)) +
  geom_bar(stat = "count") +
  labs(title = "Loan Status by Home Ownership") + xlab("Home Ownership") + ylab("Number of samples")
```

```{r}
data1 %>%
  group_by(home_ownership, loan_status) %>%
  ggplot(aes(x = home_ownership, fill = loan_status)) +
  geom_bar(stat = "count") +
  labs(title = "Loan Status by Home Ownership") + xlab("Home Ownership") + ylab("Number of samples")
```

```{r}
data1 %>% group_by(loan_status)  %>% summarise(mean(annual_inc))

data1 %>% mutate(loan_status = 2 - as.numeric(loan_status)) %>% # 0:fully paid, 1: charged off
  ggplot(aes(x = annual_inc, y = loan_status)) + 
  geom_jitter(height = .1, aes(color = factor(loan_status))) +
  labs(title = "Loan Status by Annual Income") + xlab("Annual Income") + ylab("Loan Status")
```

```{r}
data1 %>%
  group_by(purpose, loan_status) %>%
  ggplot(aes(x = purpose, fill = loan_status)) +
  geom_bar(stat = "count") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(title = "Loan Status by Purpose") + xlab("Purpose") + ylab("Number of samples")
```

The top five largest number of observations is in CA, NY, FL, TX, and NJ. There are 810 categories of different zip codes, so we might consider dropping this variable when modeling. The top five largest number of observations is with zip code 100xx, 945xx, 112xx, 606xx, 070xx.

```{r}
data1 %>%
  group_by(addr_state) %>%
  summarise(num = n()) %>%
  arrange(desc(num))

data1 %>%
  group_by(zip_code) %>%
  summarise(num = n()) %>%
  arrange(desc(num))
```

```{r}
data1 %>% group_by(loan_status)  %>% summarise(mean(dti))
```

```{r}
data1 %>% group_by(loan_status)  %>% summarise(mean(open_acc))
```

```{r}
data1 %>%
  group_by(pub_rec_bankruptcies, loan_status) %>%
  ggplot(aes(x = pub_rec_bankruptcies, fill = loan_status)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "Loan Status by Number of Public Record Bankruptcies") + 
  xlab("Number of public record bankruptcies") + ylab("Number of samples")
```

Because `addr_state` includes the geographical information of the borrower and there are too many categories of `zip_code`, we drop `zip_code`.

There 526 categories of `earliest_cr_line` and we only keep the year of this variable. After changing the variable type to numeric, we calculate the mean for 2 groups of different `loan_status`. The two mean values are 1996.905 and 1996.515. We drop `earliest_cr_line`. 

```{r}
data1 %>% 
  mutate(earliest_cr_line = sub("Jan", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Feb", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Mar", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Apr", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("May", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Jun", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Jul", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Aug", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Sep", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Oct", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Nov", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = sub("Dec", "", earliest_cr_line)) %>%
  mutate(earliest_cr_line = as.numeric(earliest_cr_line)) %>% 
  group_by(loan_status) %>%
  summarise(mean(earliest_cr_line))
```

The month of `issue_d` is not very informative so we only keep the year of this variable. After changing the variable type to numeric, we calculate the mean for 2 groups of different `loan_status`. The two mean values of `issue_d` are 2010.427 and 2010.371. We drop `issue_d`. 

```{r}
data1 %>% 
  mutate(issue_d = sub("Jan", "", issue_d)) %>%
  mutate(issue_d = sub("Feb", "", issue_d)) %>%
  mutate(issue_d = sub("Mar", "", issue_d)) %>%
  mutate(issue_d = sub("Apr", "", issue_d)) %>%
  mutate(issue_d = sub("May", "", issue_d)) %>%
  mutate(issue_d = sub("Jun", "", issue_d)) %>%
  mutate(issue_d = sub("Jul", "", issue_d)) %>%
  mutate(issue_d = sub("Aug", "", issue_d)) %>%
  mutate(issue_d = sub("Sep", "", issue_d)) %>%
  mutate(issue_d = sub("Oct", "", issue_d)) %>%
  mutate(issue_d = sub("Nov", "", issue_d)) %>%
  mutate(issue_d = sub("Dec", "", issue_d)) %>%
  mutate(issue_d = as.numeric(issue_d)) %>% 
  group_by(loan_status) %>%
  summarise(mean(issue_d))
```

`last_pymnt_d` and `last_credit_pull_d` is not very informative for the `loan_status`, so we drop them as well. Most `funded_amnt_inv` is equal to `funded_amnt`. We keep `funded_amnt` and drop `funded_amnt_inv`. Similarly, we drop `total_pymnt_inv`.

Each category of `emp_length` has a similar ratio of 2 groups of `loan_status`. There are 1070 observations with "n/a" `emp_length`. So we drop `emp_length`.

```{r}
data1 %>%
  group_by(emp_length, loan_status) %>%
  ggplot(aes(x = emp_length, fill = loan_status)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "Loan Status by Employment Length") + xlab("Employment Length") + ylab("Number of samples")

# number of observations with "n/a" employment length
length(which(data1$emp_length=="n/a", arr.ind=TRUE))
```

There is only 1 observation each for state IA, IN, and NE. We remove these 3 observations for simplicity in data training and testing.

```{r}
data1 %>% group_by(addr_state) %>% summarise(num = n()) %>% arrange(num)
state_pos <- c(which(data1$addr_state=="IA", arr.ind=TRUE), which(data1$addr_state=="IN", arr.ind=TRUE), which(data1$addr_state=="NE", arr.ind=TRUE))
data1.1 <- data1[-c(state_pos),] 
```


```{r}
data2 <- data1.1 %>% select(-zip_code, -earliest_cr_line, -issue_d, -last_pymnt_d, -last_credit_pull_d, -emp_length, - funded_amnt_inv, -total_pymnt_inv)
```

We split the data into `data.train`, `data.test`, and `data.val` with portions of 0.6, 0.2, and 0.2 respectively of data size *N*.

```{r}
N <- length(data2$loan_amnt)
n1 <- floor(.6*N)
n2 <- floor(.2*N)

set.seed(0)

idx_train <- sample(N, n1)
idx_no_train <- (which(!seq(1:N) %in% idx_train))
idx_test <- sample(idx_no_train, n2)
idx_val <- which(!idx_no_train %in% idx_test)
data.train <- data2[idx_train,]
data.test <- data2[idx_test,]
data.val <- data2[idx_val,]
```


ii) Analyses

* Various appropriate statistical methods: e.g. glmnet (and/or trees, ignore this at the moment)

We use logistic regression because we want to find the probability of a loan being charged off.

One simple multiple logistic regression is `loan_status ~ grade`.

```{r}
fit_grade <- glm(loan_status ~ grade, data.train, family = binomial)
summary(fit_grade)
```

Model 1: multiple logistic regression

```{r}
fit1 <- glm(loan_status~. - sub_grade, data.train, family=binomial)
# summary(fit1)
```

Model selection

We first fit all variables, excluding `sub_grade` for simplicity. But the fitted probabilities are extremely close to zero or one. Some backward eliminations are conducted for model selection.

```{r}
fit1.1 <- update(fit1, .~. -addr_state - purpose)
summary(fit1.1)
```

```{r}
fit1.2 <- update(fit1.1, .~. - total_pymnt - total_rec_prncp - total_rec_int - total_rec_late_fee - recoveries - last_pymnt_amnt - collection_recovery_fee - revol_bal)
summary(fit1.2)
```

```{r}
fit1.3 <- update(fit1.2, .~. -total_acc - installment - verification_status)
summary(fit1.3)
```

```{r}
fit1.4 <- update(fit1.3, .~. - dti - delinq_2yrs - open_acc)
summary(fit1.4)
```

```{r}
fit1.5 <- update(fit1.4, .~. - home_ownership - pub_rec_bankruptcies)
# Wald test
summary(fit1.5)
```

Likelihood ratio test

```{r}
# null model
fit0 <- glm(loan_status~1, data.train, family=binomial)
anova(fit0, fit1.5, test="Chisq") # reject the null model hypothesis 
```

if `funded_amnt` and `loan_amnt` is useful.

```{r}
Anova(fit1.5)
anova(update(fit1.5, .~. - funded_amnt - loan_amnt), fit1.5, test="Chisq") 
# no evidence to reject update(fit1.5, .~. - funded_amnt - loan_amnt)
```

```{r}
fit1.6 <- update(fit1.5, .~. - funded_amnt - loan_amnt)
summary(fit1.6)
```

Chisq test if `grade` is useful.

```{r}
anova(update(fit1.6, .~. - grade), fit1.6, test="Chisq") 
# no evidence to reject update(fit1.6, .~. - grade)
```

```{r}
fit1.7 <- update(fit1.6, .~. - grade)
summary(fit1.7)
```

The AIC is extremely large in fit1.7, so we use `bestglm()` to find model with the smallest AIC.

```{r}
Xy_design <- model.matrix(loan_status ~ term + int_rate + annual_inc + inq_last_6mths + pub_rec + revol_util +0, data.train) 
Xy <- data.frame(Xy_design, data.train$loan_status) 

fit.1b <- bestglm::bestglm(Xy, family = binomial, method = "exhaustive", IC="AIC", nvmax = 10)
# fit.1b$BestModels
```


Model 2: LASSO in logistic regression: elastic net for classification

```{r}
X <- model.matrix(loan_status~addr_state + loan_amnt + funded_amnt + term + int_rate + grade + annual_inc + inq_last_6mths + pub_rec + revol_util + dti + delinq_2yrs + open_acc + home_ownership + pub_rec_bankruptcies, data.train)[, -1]
Y <- data.train$loan_status
fit2.cv <- cv.glmnet(X, Y, alpha=1, family="binomial", nfolds = 10, type.measure = "deviance")
```

lambda.min

```{r}
coef.min <- coef(fit2.cv, s="lambda.min") 
coef.min <- coef.min[which(coef.min !=0), ]
as.matrix(coef.min)
```

refit logistic regression:

```{r}
# rownames(as.matrix(coef.min))

fit2.lgt.1 <- glm(loan_status~addr_state + loan_amnt + term + int_rate + grade + annual_inc + inq_last_6mths + pub_rec + revol_util + dti + delinq_2yrs + open_acc + pub_rec_bankruptcies, family=binomial, data=data.train)
summary(fit2.lgt.1)

Anova(fit2.lgt.1)
```

See if we can drop all of `loan_amnt`, `dti`, `delinq_2yrs`, `open_acc`, and `pub_rec_bankruptcies`.

```{r}
fit2.lgt.2 <- glm(loan_status~addr_state + term + int_rate + grade + annual_inc + inq_last_6mths + pub_rec + revol_util, family=binomial, data.train)
anova(fit2.lgt.2,  fit2.lgt.1,  test="Chisq")
```

No evidence to keep `loan_amnt`, `dti`, `delinq_2yrs`, `open_acc`, and `pub_rec_bankruptcies`.

```{r}
Anova(fit2.lgt.2)
```

```{r}
fit2.lgt.3 <- update(fit2.lgt.2, .~. - grade)
Anova(fit2.lgt.3)
```

* Comparisons various models

Compare fit1.7 and fit2.lgt.3

Get the fitted probabilities using the testing data

```{r}
fit1.fitted.test <- predict(fit1.7, data.test, type = "response")
fit2.fitted.test <- predict(fit2.lgt.3, data.test, type = "response")

data.frame(fit1.fitted.test, fit2.fitted.test)[1:10, ]
```

```{r}
fit1.test.roc <- pROC::roc(data.test$loan_status, fit1.fitted.test)
fit2.test.roc <- pROC::roc(data.test$loan_status, fit2.fitted.test)
```

```{r}
pROC::auc(data.test$loan_status, fit1.fitted.test)
fit2.fitted.val <- predict(fit2.lgt.3, data.val, type="response")
pROC::auc(data.val$loan_status, fit2.fitted.val)
```

* Final model(s)

Classification

We estimate that the loss ratio of picking up a bad loan (false positive) to that of missing a good loan (false negative) is about 2 to 1, i.e., $a_{0,1}$/$a_{1,0}$ = 2. According to the Bayes' Rule, we pick the threshold of 1/3.

Confusion matrix

```{r}
fit1.f <- glm(loan_status ~ term + int_rate + annual_inc + inq_last_6mths + pub_rec + revol_util, data.val, family=binomial)
fit1.pred <- ifelse(fit1.f$fitted > 2/3, "1", "0")
table(fit1.pred, data.val$loan_status) #0: charged off, 1: fully paid
```

```{r}
fit2.f <- glm(loan_status ~ addr_state + term + int_rate + annual_inc + inq_last_6mths + pub_rec + revol_util, data.val, family=binomial)
fit2.pred <- ifelse(fit2.f$fitted > 2/3, "1", "0")
table(fit2.pred, data.val$loan_status) #0: charged off, 1: fully paid
```

$2\cdot177 > 297$, indicating that our model will increase Lending Club's profit under the estimated loss ratio of 2:1.

iii) Conclusion

* Summarize results and the final model

The two models do not vary much on their performances. Since fit1.7 is simpler (does not include the 49-category `addr_state`) and its AIC is relatively smaller, we choose `loan_status ~ term + int_rate + annual_inc + inq_last_6mths + pub_rec + revol_util` as our final model.

* Caveats
* Final recommendations

Based on the information we gathered of Lending Club's [business model](https://en.wikipedia.org/wiki/LendingClub#Business_model), it earns money by charging borrowers origination fee and investors service fee. Varying from the credit grade, the origination fee ranges from *1.1%* to *5%* of the loan amount and the service fee is *1%* of borrower's payment. The average loan amount is *$11267.29*. For each transaction, LC's average profit ranges from *$236.62* to *$563.36*. For 7795 transactions, LC's profit may increase amount ranging from *$13,487* to *$32,111* applying our model under the estimate loss ratio of 2:1.

Maintain a good descriptive flow in the text of your report. Use Appendices to display lengthy output. 

```{r}
data1 %>% group_by(grade) %>% summarise(num = n())
```

iii) Appendix
	
* All your R code (code without comments is no good!) if you are not using `rmd` format.
* Any thing necessary to keep but for which you don’t want them to be in the main report.

